#' calibrate
#'
#' This function calibrates items in a test and adds comments and flags of
#' levels of priority for review. This is associated with test named 'test'.
#' If save_xlsx is TRUE, an Excel file with a summary of the above information
#' will be saved in 'results' folder in the working directory.
#'
#' @param test Name of the test.
#' @param respDf Dataframe with pid, covariables (e.g,, DIF variable), and
#' responses. Default is NULL where Excel file with name 'test' in 'data'
#' folder is used.
#' @param regrNmVec Vector of character regressors' names.
#' @param pid Name of candidates' ID variable.
#' @param n_cov Number of covariates before responses.
#' @param n_dims Vector of numbers of responses the dimensions have.
#' Default is NULL. Define this vector if multi-dimensional model is to be run,
#' e.g., c(30, 45). Also should define this if there are variables after
#' response columns, e.g., 30.
#' @param dim_names Vector of the dimensions' names. Default is NULL.
#' Define this vector if multi-dimensional model is to be run.
#' @param keyDf Dataframe of 'Item', 'Key', and 'Max_score' (add Key2 if double key).
#' @param quick TRUE if empirical error is not needed. Default is TRUE
#' @param delete Vector of orders or labels of items to be removed. Default is NULL.
#' @param dbl_key TRUE if any item has polytomous scoring. Default is NULL.
#' @param anchor TRUE when anchor is to be done. Default is FALSE.
#' @param section_extr Extra sections to be added to 'test.cqc' file in
#' 'input' folder. Default is NULL.
#' @param easy Threshold to flag easy items. Default is 90 (percent correct).
#' @param hard Threshold to flag hard items. Default is 10 (percent correct).
#' @param iRst Threshold to flag low item-rest correlation statistics.
#' Default is 0.11.
#' @param fit_w Threshold to flag large weighted item fit statistics.
#' Default is 1.1.
#' @param fit_uw Threshold to flag large unweighted item fit statistics.
#' Default is 1.2.
#' @param dFallThr Ability on last bin above which falling distractor is flagged.
#' Default is 0.5.
#' @param dRiseThr Ability on last bin below which rising distractor is unflagged.
#' Default is 0.1.
#' @param numAbilGrps Number of ability groups. Default is NULL.
#' @param recode_poly TRUE if polytomous items have non-continuous scores.
#' Note that code in c('r','R','m','M','9','x','X','.','',' ',NA) will be kept
#' intact while other codes will be down scored to be continuous. Default is FALSE.
#' @param missCode2Conv Response codes to convert to embedded/trailing missing
#' ('M' or 'R'). Use freq_resps_cat() to explore response distribution
#' and determine which codes need recoding. Commonly found missing symbols
#' are '@@' (less or more), 7, 8, 9, 88, 99, '.', '-', '', NA. Default is NULL
#' when responses have been recoded.
#' @param filetype Format for input dataset. Default is 'sav'. Also support
#' csv format.
#' @param slope Slope to multiply ability estimates. Default is NULL
#' @param intercept Value/intercept to add to ability estimates. Default is NULL.
#' @param extrapolation Whether to extrapolate the minimum and maximum estimates.
#' Default is FALSE.
#' @param save_xlsx Whether to save summary file. Default is TRUE (single test).
#' @param est_type Type of ability estimate to use for score equivalence table,
#' 'wle' or 'mle'. 'wle' is commonly used. Default is NULL (equiv table not needed).
#' @param sparse_check Whether to check response column sparsity in general or
#'  regarding any DIF variable category. Default is FALSE. If TRUE, sparse
#'  response columns will be removed.
#' @param CCCip2Wd TRUE if to save CCC and item-person map to a Word file. Default
#' is FALSE.
#' @param pweight Variable name of person weights in response dataframe. Should
#' be specified if weight is used for modeling. Default is NULL.
#' @param ancDf Dataframe of 'Item' and 'Delta' for anchors. Default is NULL.
#'  If NULL, an xxx_anc.txt file with anchor tbl (Item, Delta) should be put
#'  in 'input' folder beforehand. Check output 'xxx_anc.txt' file from previous run
#'  for correct anchor order, especially for polytomous items with step parameters.
#' @examples
#' # Not run
#' # calibrate(respDf=racp, test='RACP', pid="V1", n_cov=1, keyDf=cd,
#' # delete=c(3,4,5,36), dbl_key=list(`7`=c(1,3), `9`=c(3,4)))
#' @export

calibrate <- function(test, respDf=NULL, keyDf, pid, n_cov,
                      regrNmVec=NULL, n_dims=NULL, dim_names=NULL,
                      quick=TRUE, delete=NULL,
                      dbl_key=NULL, anchor=FALSE, section_extr=NULL,
                      easy=90, hard=10, iRst=.11, fit_w=1.1, fit_uw=1.2,
                      dFallThr=.5, dRiseThr=.1,
                      numAbilGrps=NULL, recode_poly=FALSE,
                      missCode2Conv=NULL,
                      filetype='sav', slope=NULL, intercept=NULL,
                      extrapolation=FALSE, save_xlsx=TRUE, est_type=NULL,
                      sparse_check=FALSE, CCCip2Wd=FALSE,
                      pweight=NULL, ancDf=NULL){
  options(warn=-1)

  cat('\n============', if (anchor) 'Anchoring' else 'Calibrating', ':', test, '============\n\n')

  # read data
  save_data <- TRUE
  if (is.null(respDf)) {
    cat('Reading data...\n')
    if (filetype == 'sav') {
      respDf <- haven::read_sav(paste0('data/', test, '.sav'))
      label_csv <- paste0('data/', test, '_labels.csv')
      if (file.exists(label_csv)){
        nms <- read.csv(label_csv)$iLabel
        names(respDf) <- nms
      }
    } else if (filetype == 'csv') {
      respDf <- read.csv(paste0('data/', test, '.csv'))
    } else {
      stop('Data must use sav or csv.')
    }
    save_data <- FALSE
  }

  # check input
  cat('Checking inputs...\n')
  if (!all(c(pid, regrNmVec) %in% names(respDf))) {
    stop('Pid or regressor is not in respDf column names!')
  }
  if (length(respDf[[pid]]) != length(unique(respDf[[pid]]))){
    stop('Please use unique pid for cases!')
  }
  if (!is.null(ancDf)){
    if (!all(names(ancDf) %in% c('Item', 'Delta'))){
      stop('ancDf should have names as \'Item\' and \'Delta\'!')
    }
  }
  if (!all(names(keyDf) %in% c('Item', 'Key', 'Max_score', 'Key2'))){
    stop('keyDf should have names as \'Item\', \'Key\', \'Max_score\', \'Key2 (if double key)\'!')
  }

  # calculate arguments
  cat('Using default arguments if not given...\n')
  if (is.null(n_dims)) n_dims <- ncol(respDf) - n_cov
  labels <- keyDf$Item
  names(respDf)[(n_cov+1):(n_cov+sum(n_dims))] <- labels

  # ####### check folders that may contain files related to 'test'
  cat('Move existing files with test name into new folder if any...\n')
  if (is.null(ancDf)){
    folders_mov <- c('output', 'results')
  } else {
    folders_mov <- c('input', 'output', 'results')
  }
  map(folders_mov, ~move_into_folder(folder=.x, test=test))

  # ####### preprocess data
  poly_key <- ifelse(any(keyDf$Max_score > 1), TRUE, FALSE)

  if (poly_key){
    cat('Checking polytomou-score items; recode if score are not continuous...\n')
    if (recode_poly) {
      respDf <- poly_recode(keyDf, respDf, n_cov, c('r','R','m','M','9','x','X','.','',' ',NA))
    }
  }

  if (sparse_check){
    cat('Checking and removing items without data on any item or DIF variable categories...\n')
    processed <- sparse_data_process(test, respDf, keyDf, labels, n_cov, n_dims,
                                     c('.', 'r', 'R', 'x', 'X', '', ' '), NULL)
    respDf <- processed[['data']]
    n_dims <- processed[['n_dims']]
    keyDf <- processed[['keys']]
    labels <- processed[['labels']]
  }

  # recode data
  if (!is.null(missCode2Conv)){
    cat('Recoding embedded & trailing missing in responses to M & R...\n')
    for (i in 1:length(n_dims)){
      if (i==1){
        respDf <- miss_recode(respDf, n_cov+1, n_cov+n_dims[[i]], 'M','R',
                  NA, missCode2Conv, F, F)
      }
      else {
        respDf <- miss_recode(respDf, n_cov+sum(n_dims[1:(i-1)])+1,
                  n_cov+sum(n_dims[1:i]), 'M','R', NA,
                  missCode2Conv, F, F)
      }
    }
  }

  # find out deleted item order if delete is item labels
  if (!is.null(delete)){
    if (typeof(delete) == "character"){
      delete <- which(labels %in% delete)
    }
  }

  # save data
  if (save_data) {
    cat('Saving data into csv and sav...\n')
    write.csv(respDf, paste0('data/', test, '.csv'), row.names=FALSE)
    # for .sav data
    tryCatch({
      haven::write_sav(respDf, paste0('data/', test, '.sav'))
    },
    error = function(e) {
      data_sav <- respDf
      iSPSS <- paste0('V', 1:ncol(data_sav))
      names(data_sav)[1:ncol(data_sav)] <- iSPSS

      N1 <- sum(n_cov, length(labels))
      if (ncol(respDf)==N1){
        iLabel <- c(names(respDf)[1:n_cov], labels)
      } else {
        iLabel <- c(names(respDf)[1:n_cov], labels, names(respDf)[(N1+1):ncol(respDf)])
      }

      write.csv(
        tibble(iSPSS = iSPSS, iLabel = iLabel),
        paste0('data/', test, '_labels.csv'),
        row.names = FALSE
      )
      haven::write_sav(data_sav, paste0('data/', test, '.sav'))
    })
  }

  # prepare arguments
  cat('Preparing ConQuest control file...\n')
  prep <- df_key_lab_args(test, respDf, pid, n_cov, sum(n_dims), NULL,
                          regrNmVec, section_extr, labels, anchor, pweight)
  if (length(n_dims) > 1){
    if(is.null(dim_names)) stop('Please set dimension names \'dim_names\'!')
    if (poly_key) scrs <- 0:max(keyDf$Max_score) else scrs <- 0:1
    prep[['section_extr']] <- prep[['section_extr']] |>
      c(section_dim(scrs=scrs, n_dims=n_dims, dim_names=dim_names))
  }

  # ####### process anchor file
  if (anchor) {
    if (is.null(ancDf) || poly_key){ # poly: use previous, or manually prepare
      # read from 'input' folder
    } else {
      cat('Processing anchor file...\n')
      anchor_process(test, respDf, keyDf, labels, delete, poly_key, n_cov, n_dims, ancDf)
    }
  }

  # ####### calibrate
  cat('Calibrating test items...\n')
  lab_cqc(test=test, keys=keyDf, run=NULL, run_ls=NULL,
      codes=prep$codes, pid_cols=prep$pid_cols, resps_cols=prep$resps_cols,
      quick=quick, delete=delete, dbl_key=dbl_key, poly_key=poly_key,
      anchor=anchor, step=FALSE, regr_ls=prep$regr_ls,
      section_extr=prep$section_extr,
      DIFVar=NULL, DIFVar_cols=prep$DIFVar_cols, poly_catgrs=NULL,
      poly_facet=FALSE, poly_group=FALSE,
      pweight=pweight, pw_cols=prep$pw_cols)

  # ####### read CQS output for summary
  cat('Reading CQS file...\n')
  cqs <- conquestr::ConQuestSys(paste0('output/', test, ".CQS"))
  saveRDS(cqs, paste0('output/', test, "_CQS.rds"))

  # ####### check: Convergence
  cat('Checking convergence...\n')
  check_convergence(cqs, test)

  if (anchor){
    # check: input .anc file vs. output .anc file
    if (!poly_key){
      anchor_dif <- read.table(paste0('input/', test, '_anc.txt')) |>
        dplyr::select(anchor=V3, input=V2) |>
        left_join(
          read.table(paste0('output/', test, '_anc.txt')) |>
            mutate(output=V2, anchor=str_c(V3,V5,V6)) |>
            dplyr::select(anchor, output),
          by = "anchor"
        ) |>
        mutate(
          dif = input - output,
          anchor = str_remove_all(anchor, '[(//)(/*)]')
        ) |>
        dplyr::filter(abs(dif) > 0.0001)
      if (nrow(anchor_dif) != 0 & !poly_key){
        print(anchor_dif)
        stop('Anchor order was messed up! Check printed difference above.')
      }
    } else {
      cat('Please ensure equality of anchor values in input and output folders.\n')
    }

    # get equivalence table
    if (!is.null(est_type)) {
      cat('Generating equivalence table...\n')
      equiva_tbl(test, est_type, slope, intercept, extrapolation)
    }
    # est_cas(test)
    rm(cqs)

    # point users to files of varying purposes
    writeLines(c(
      paste0('\n===== Output Files\n'),
      paste0('Anchoring and scaling of ', toupper(test), ':'),
      if (!is.null(est_type)) {
        paste0('\tScore equivalence table:\t', 'results/', 'eqv_tbl_', test, '.xlsx')
      },
      paste0('\tRaw and logit score table:\t', 'output/', test, '_cas',  '.xls')
    ))
  } else { # summarize item calibration
    # ####### check: Option frequencies
    cat('Checking option frequencies...\n')
    tryCatch(
      {
        check_freq_resps_cat(test, respDf[(n_cov+1):(n_cov+sum(n_dims))])
      },
      error = function(e){
        invisible()
      }
    )

    # ####### TODO: Plotting, summary stats for multi-dim models #######
    if (length(n_dims)>1) return('Results are in output folder.')

    # ####### CCC of categories and scores
    cat('Producing Category Characteristic Curve (CCC)...\n')
    # determine whether to use wle or pv1
    n_min <- min(map_int(respDf[-c(1:n_cov)], ~length(str_remove_all(na.omit(.x), 'R'))))
    abilEst2use <- ifelse(n_min >= 200, 'pv1', 'wle')
    plot_data <- CCC_ipMap(test, cqs, abilEst2use, numAbilGrps, poly_key)
    ccc_data <- plot_data[['ccc_data']]
    iType <- plot_data[['itype']]

    # save CCC, imap to Word file
    if (CCCip2Wd) {
      cat('Saving CCC and ipMap to Word file...\n')
      rmd_file <- system.file("rmd", "CCC_ipMap.Rmd", package = "RaschKit")
      rmarkdown::render(
        rmd_file,
        params = list(test=test, plot_data=plot_data),
        output_file = str_c(test, '_CCC_ipMap', '.docx'),
        output_dir = here::here('output'),
        quiet = TRUE
      )
    }

    # ####### item summary
    cat('Putting together item analysis summaries...\n')
    smry <- itn_summary(test, easy, hard, iRst, fit_w, fit_uw,
              dFallThr, dRiseThr, ccc_data, iType, keyDf)
    rm(cqs)
    if (save_xlsx){
      file_saved <- paste0('results/', paste0('itn_', test, '.xlsx'))
      writexl::write_xlsx(smry, file_saved)

      # point users to files of varying purposes
      writeLines(c(
        paste0('\n===== Output Files\n'),
        paste0('Item calibration of ', toupper(test), ':'),
        paste0('\tCQ output:\t', 'output/', ' (Files with \'', test, '\' in name)'),
        if (recode_poly & file.exists(paste0('data/', test, '_recode_score.csv'))){
          paste0('\tScore recoding:\t', 'data/', test, '_recode_score.csv')
        },
        if (save_data){
          paste0('\tData saved:\t', 'data/', test, '.xlsx\n\t\t\t',
               'data/', test, '.sav')
        },
        paste0('\tConverge check:\t', 'output/', test, '_convergence_check.pdf'),
        paste0('\tQA:\t\t', 'output/', test, '_Frequency_check.xlsx'),
        paste0('\tCCC:\t\t', 'output/', test, '_CCC.pdf'),
        if (CCCip2Wd){
          paste0('\tCCC_ipMap:\t', 'output/', test, '_CCC_ipMap', '.docx')
        },
        paste0('\tsummary:\t', 'results/', 'itn_', test, '.xlsx')
      ))
    } else {
      cat('Calibration of', test, 'completed.\n')
      return(smry)
    }
  }

}
